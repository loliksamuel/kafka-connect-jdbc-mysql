= Kafka Connect JDBC test environment

This Docker Compose can be used to spin up an environment in which to explore and test the Kafka Connect JDBC source connector. 

see article at https://www.confluent.io/blog/kafka-connect-deep-dive-jdbc-source-connector

Current environment
|=====================================================================
|Confluent Platform | 5.1.0
|MySQL              | 8.0.13
|Postgres           | 11.1
|MS SQL Server      | 2017 (RTM-CU13) (KB4466404) - 14.0.3048.4 (X64)
|Oracle             | 12.2.0.1.0  (from github)
|=====================================================================

== Get inside containers


[source,bash]
----
docker-compose up -d

KAFKA KSQL-(guidelines for a basic KSQL server:
           4 cores
           32 GB RAM
           100 GB SSD
           1 Gbit network)
docker-compose exec ksql-cli ksql http://ksql-server:8088

KAFKA CONNECT
docker exec -it kafka-connect-jdbc-mysql_kafka-connect_1 bash
cd /usr/share/java/kafka-connect-jdbc/jars
 ls |grep  ojdbc8.jar

MySQL
MYSQL_ROOT_PASSWORD=Admin123 & echo $MYSQL_ROOT_PASSWORD
docker-compose exec mysql bash -c 'mysql -u root -p$MYSQL_ROOT_PASSWORD'
 or
docker exec -it bxxxx bash  --user=root --password=$MYSQL_ROOT_PASSWORD
mysql>ALTER USER 'root' IDENTIFIED WITH mysql_native_password BY '123';
mysql> show databases;
mysql> use demo;
mysql>select id,first_name,last_name, username, company,created_date,update_ts from accounts;
mysql>exit

POSTGRES
docker-compose exec postgres bash -c 'psql -U $POSTGRES_USER $POSTGRES_DB'

ORACLE
docker-compose exec oracle bash -c 'sqlplus sys/$ORACLE_PWD@localhost:1521/ORCLCDB as sysdba'

MS SQL SERVER
docker-compose exec mssql bash -c '/opt/mssql-tools/bin/sqlcmd -l 30 -S localhost -U sa -P $SA_PASSWORD'
----


==  Connector Configuration

* MySQL-add connector with a post
** curl -X GET http://localhost:8083/connectors – returns a list with all connectors in use
** curl -X GET http://localhost:8083/connectors/{name} – returns details about a specific connector
** curl -X DELETE http://localhost:8083/connectors/{name} – delete  specific connector
** curl -X POST http://localhost:8083/connectors – creates a new connector; the request body should be a JSON object containing a string name field and an object config field with the connector configuration parameters
** curl -X POST http://localhost:8083/connectors -H "Content-Type: application/json" -d "@data_oracle08.json"

==  Play with ksql

https://docs.confluent.io/current/connect/kafka-connect-jdbc/source-connector/source_config_options.html
[source,bash]
----
create a connector(using bulk is not a best practice, Valid Values: [, bulk, timestamp, incrementing, timestamp+incrementing])
or
curl -X POST http://localhost:8083/connectors -H "Content-Type: application/json" -d '{
        "name": "jdbc_source_mysql_08",
        "config": {
                "connector.class": "io.confluent.connect.jdbc.JdbcSourceConnector",
                "connection.url": "jdbc:mysql://mysql:3306/demo",
                "connection.user": "connect_user",
                "connection.password": "asgard",
                "topic.prefix": "mysql-08-",
                "mode":"bulk",
                "batch.max.rows":100,
                "table.whitelist" : "demo.accounts",
                "poll.interval.ms" : 360000
                }
        }'

 response:
        {"name":"jdbc_source_mysql_01","config":{"connector.class":"io.confluent.connect.jdbc.JdbcSourceConnector","connection.url":"jdbc:mysql://mysql:3306/demo","connection.user":"connect_user","connection.password":"asgard","topic.prefix":"mysql-01-","mode":"bulk","poll.interval.ms":"10000","name":"jdbc_source_mysql_01"},"tasks":[],"type":null}
curl -s -X GET http://localhost:8083/connectors/|jq
curl -s -X GET http://localhost:8083/connectors/jdbc_source_mysql_01|jq
curl -s -X GET "http://localhost:8083/connectors/jdbc_source_mysql_08/status"|jq

ksql> PRINT 'mysql-01-accounts' FROM BEGINNING;
u will see the content of db in the topics
insert to db
mysql>INSERT INTO demo.accounts
(`id`,
`first_name`,
`last_name`,
`username`,
`company`,
`created_date`) VALUES
(30,
'lolik10',
'samuel10',
'loliksamuel',
'zim',
'2019-03-03');
after 10 sec, u will see it in the topic

create another connector in mode : timestamp
curl -X POST http://localhost:8083/connectors -H "Content-Type: application/json" -d '{
        "name": "jdbc_source_mysql_08",
        "config": {
                "connector.class": "io.confluent.connect.jdbc.JdbcSourceConnector",
                "connection.url": "jdbc:mysql://mysql:3306/demo",
                "connection.user": "connect_user",
                "connection.password": "asgard",
                "topic.prefix": "mysql-08-",
                "mode":"timestamp",
                "table.whitelist" : "demo.accounts",
                "timestamp.column.name": "UPDATE_TS",
                "validate.non.null": false
                }
        }'



mysql> INSERT INTO demo.accounts (`id`, `first_name`, `last_name`, `username`, `company`, `created_date`) VALUES (30, 'lolik10', 'samuel10', 'loliksamuel', 'zim', '2019-03-03');
Query OK, 1 row affected (0.00 sec)
verify after 1 sec, that u see it in the topic

mysql>update demo.accounts set first_name = 'lolik311' where id=31;
verify after 1 sec, that u see it in the topic

mysql>delete from demo.accounts where id=31;
verify that jdbc connector does not support delete oparations. if u need it than consider use cdc transaction-log connector.

ksql> CREATE TABLE users (registertime BIGINT,  userid VARCHAR, gender VARCHAR, regionid VARCHAR)  WITH (KAFKA_TOPIC = 'mysql-08-accounts',  VALUE_FORMAT='JSON', KEY = 'userid');
ksql>SHOW | LIST tables;
ksql> DESCRIBE [EXTENDED] users;
ksql>DROP TABLE  IF EXISTS  users;
ksql>SHOW | LIST tables;
ksql>CREATE TABLE mysql-08-accounts-table (id INT,  company VARCHAR)  WITH (KAFKA_TOPIC = 'mysql-08-accounts',  VALUE_FORMAT='JSON', KEY = 'id');
ksql>CREATE TABLE accountGroupByTable WITH (PARTITIONS=1,REPLICAS=1) AS select last_name, count(*) as count from demo.accounts group by last_name;
ksql>CREATE TABLE accountGroupByTable2  (last_name string, COUNT bigint) WITH (kafka_topic='mysql-08-accounts', value_format='JSON') ;
ksql>describe extended accountGroupByTable2; --see the columns & how many massages
ksql>select * from accountGroupByTable2;
note u do not see anything. it is because no new data is inserted. let's insert in different window...
mysql> INSERT INTO demo.accounts (`id`, `first_name`, `last_name`, `username`, `company`, `created_date`) VALUES (40, 'lolik40', 'samuel', 'loliksamuel', 'zim', '2019-03-03');
Query OK, 1 row affected (0.00 sec)
verify after 1 sec, that u see it in the table accountGroupByTable2

ksql>CREATE TABLE accountGroupByTable  (usertimestamp BIGINT, user_id VARCHAR, gender VARCHAR, region_id VARCHAR) KAFKA_TOPIC = 'mysql-08-accounts',KEY = 'user_id');
ksql>CREATE STREAM accountGroupByStream (last_name string, COUNT bigint) WITH (kafka_topic='mysql-08-accounts', value_format='JSON') ;
CREATE TABLE users (UPDATE_TS BIGINT, id VARCHAR, first_name VARCHAR, company VARCHAR) KAFKA_TOPIC = 'mysql-08-accounts', KEY = 'id');
ksql>SHOW | LIST topics;
ksql>SHOW | LIST streams;
ksql>SHOW | LIST tables;
ksql>SHOW | LIST queries;
ksql>SHOW | LIST functions;
ksql>SHOW | LIST properties;
ksql>print 'ACCOUNTGROUPBY' FROM BEGINNING;
???
ksql>DROP TABLE [IF EXISTS] table_name [DELETE TOPIC];
ksql>DROP STREAM [IF EXISTS] stream_name [DELETE TOPIC];
ksql> PRINT 'mysql-01-accounts' FROM BEGINNING
ksql> CREATE STREAM ACCOUNTS WITH (KAFKA_TOPIC='mysql-06X-accounts', VALUE_FORMAT='AVRO');
ksql> SELECT ROWKEY, ID, FIRST_NAME + ' ' + LAST_NAME FROM ACCOUNTS;
----

* Postgres
+
[source,bash]
----
curl -X POST http://localhost:8083/connectors -H "Content-Type: application/json" -d '{
        "name": "jdbc_source_postgres_01",
        "config": {
                "connector.class": "io.confluent.connect.jdbc.JdbcSourceConnector",
                  "connection.url": "jdbc:postgresql://postgres:5432/postgres",
                "connection.user": "connect_user",
                "connection.password": "asgard",
                "topic.prefix": "postgres-01-",
                "mode":"bulk",
                "poll.interval.ms" : 3600000,
                "query" :"select * from accounts where company IN(\"Marvin Inc\", \"Wiza Inc\")"
                }
        }'


----

* Oracle
+
[source,bash]
----
cp ojdbc8.jar
docker cp /db-leach/jdbc/lib/ojdbc8.jar kafka-connect-jdbc-mysql_kafka-connect_1:/usr/share/java/kafka-connect-jdbc
curl -X POST http://localhost:8083/connectors -H "Content-Type: application/json" -d '{
                "name": "jdbc_source_oracle_01",
                "config": {
                        "connector.class": "io.confluent.connect.jdbc.JdbcSourceConnector",
                        "connection.url": "jdbc:oracle:thin:@oracle:1521/ORCLPDB1",
                        "connection.user": "connect_user",
                        "connection.password": "asgard",
                        "topic.prefix": "oracle-01-",
                        "table.whitelist" : "NUM_TEST",
                        "mode":"bulk",
                        "poll.interval.ms" : 3600000
                        }
                }'
----

* MS SQL Server
+
[source,bash]
----
curl -X POST http://localhost:8083/connectors -H "Content-Type: application/json" -d '{
                "name": "jdbc_source_mssql_01",
                "config": {
                        "connector.class": "io.confluent.connect.jdbc.JdbcSourceConnector",
                        "connection.url": "jdbc:sqlserver://mssql:1433;databaseName=demo",
                        "connection.user": "connect_user",
                        "connection.password": "Asgard123",
                        "topic.prefix": "mssql-01-",
                        "table.whitelist" : "demo..num_test",
                        "mode":"bulk",
                        "poll.interval.ms" : 3600000
                        }
                }'
----
